{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/keras/legacy/layers.py:429: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `sum`, `concatenate`, etc.\n",
      "  warnings.warn('The `merge` function is deprecated '\n",
      "/usr/local/lib/python2.7/dist-packages/keras/legacy/layers.py:66: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  warnings.warn('The `Merge` layer is deprecated '\n",
      "/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.py:86: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=[<tf.Tenso...)`\n",
      "  '` call to the Keras 2 API: ' + signature)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38951745"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras.losses import binary_crossentropy\n",
    "import numpy.random as rng\n",
    "import numpy as np\n",
    "import dill as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def W_init(shape,name=None):\n",
    "    \"\"\"Initialize weights as in paper\"\"\"\n",
    "    values = rng.normal(loc=0,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)\n",
    "#//TODO: figure out how to initialize layer biases in keras.\n",
    "\n",
    "def b_init(shape,name=None):\n",
    "    values=rng.normal(loc=0.5,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)\n",
    "\n",
    "input_shape = (105, 105, 1)\n",
    "left_input = Input(input_shape)\n",
    "right_input = Input(input_shape)\n",
    "#build convnet to use in each siamese 'leg'\n",
    "convnet = Sequential()\n",
    "convnet.add(Conv2D(64,(10,10),activation='relu',input_shape=input_shape,kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(Conv2D(128,(7,7),activation='relu',kernel_regularizer=l2(2e-4),bias_initializer=b_init))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(Conv2D(128,(4,4),activation='relu',kernel_regularizer=l2(2e-4),bias_initializer=b_init))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(Conv2D(256,(4,4),activation='relu',kernel_regularizer=l2(2e-4),bias_initializer=b_init))\n",
    "convnet.add(Flatten())\n",
    "convnet.add(Dense(4096,activation=\"sigmoid\",kernel_regularizer=l2(1e-3),bias_initializer=b_init))\n",
    "#encode each of the two inputs into a vector with the convnet\n",
    "encoded_l = convnet(left_input)\n",
    "encoded_r = convnet(right_input)\n",
    "#merge two encoded inputs with the l1 distance between them\n",
    "L1_distance = lambda x: K.abs(x[0]-x[1])\n",
    "both = merge([encoded_l,encoded_r], mode = L1_distance, output_shape=lambda x: x[0])\n",
    "prediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(both)\n",
    "siamese_net = Model(input=[left_input,right_input],output=prediction)\n",
    "optimizer = SGD(0.00004,momentum=0.65,nesterov=True,decay=0.0003)\n",
    "#//TODO: get layerwise learning rates and momentum annealing scheme described in paperworking\n",
    "siamese_net.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n",
    "\n",
    "siamese_net.count_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(964, 20, 105, 105)\n"
     ]
    }
   ],
   "source": [
    "with open( \"/home/soren/keras-oneshot/train.pickle\",\"r\") as f:\n",
    "    (X,y,c) = pickle.load(f)\n",
    "\n",
    "with open( \"/home/soren/keras-oneshot/val.pickle\",\"r\") as f:\n",
    "    (Xval,yval,cval) = pickle.load(f)\n",
    "print(X.shape)\n",
    "a=siamese_net.predict([X[1,1,:,:].reshape(1,105,105,1),X[7,1,:,:].reshape(1,105,105,1) ])\n",
    "b=siamese_net.predict([X[7,1,:,:].reshape(1,105,105,1),X[1,1,:,:].reshape(1,105,105,1) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Siamese_Loader:\n",
    "    \"\"\"For loading batches and testing tasks to a siamese net\"\"\"\n",
    "    def __init__(self,Xtrain,Xval=None):\n",
    "        if Xval is None:\n",
    "\t\tself.Xval = Xtrain \n",
    "\telse:\n",
    "       \t\tself.Xval = Xval# / Xval.max()\n",
    "        self.Xtrain = Xtrain# / Xtrain.max()\n",
    "\n",
    "\n",
    "        self.n_classes,self.n_examples,self.w,self.h = Xtrain.shape\n",
    "        self.n_val,self.n_ex_val,_,_ = self.Xval.shape\n",
    "\n",
    "    def get_batch(self,n):\n",
    "        \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
    "        categories = rng.choice(self.n_classes,size=(n,),replace=False)\n",
    "        pairs=[np.zeros((n, self.h, self.w,1)) for i in range(2)]\n",
    "        targets=np.zeros((n,))\n",
    "        targets[n//2:] = 1\n",
    "        for i in range(n):\n",
    "            category = categories[i]\n",
    "            idx_1 = rng.randint(0,self.n_examples)\n",
    "            pairs[0][i,:,:,:] = self.Xtrain[category,idx_1].reshape(self.w,self.h,1)\n",
    "            idx_2 = rng.randint(0,self.n_examples)\n",
    "            #pick images of same class for 1st half, different for 2nd\n",
    "            category_2 = category if i >= n//2 else (category + rng.randint(1,self.n_classes)) % self.n_classes\n",
    "            pairs[1][i,:,:,:] = self.Xtrain[category_2,idx_2].reshape(self.w,self.h,1)\n",
    "        return pairs, targets\n",
    "\n",
    "    def make_oneshot_task(self,N):\n",
    "        \"\"\"Create pairs of test image, support set for testing N way one-shot learning. \"\"\"\n",
    "        categories = rng.choice(self.n_val,size=(N,),replace=False)\n",
    "        indices = rng.randint(0,self.n_ex_val,size=(N,))\n",
    "        true_category = categories[0]\n",
    "        ex1, ex2 = rng.choice(self.n_examples,replace=False,size=(2,))\n",
    "        test_image = np.asarray([self.Xval[true_category,ex1,:,:]]*N).reshape(N,self.w,self.h,1)\n",
    "        support_set = self.Xval[categories,indices,:,:]\n",
    "        support_set[0,:,:] = self.Xval[true_category,ex2]\n",
    "        support_set = support_set.reshape(N,self.w,self.h,1)\n",
    "        pairs = [test_image,support_set]\n",
    "        targets = np.zeros((N,))\n",
    "        targets[0] = 1\n",
    "        return pairs, targets\n",
    "    \n",
    "    def test_oneshot(self,model,N,k,verbose=0):\n",
    "        \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
    "        pass\n",
    "        n_correct = 0\n",
    "        if verbose:\n",
    "            print(\"Evaluating model on {} unique {} way one-shot learning tasks ...\".format(k,N))\n",
    "        for i in range(k):\n",
    "            inputs, targets = self.make_oneshot_task(N)\n",
    "            probs = model.predict(inputs)\n",
    "            if np.argmax(probs) == 0:\n",
    "                n_correct+=1\n",
    "        percent_correct = (100.0*n_correct / k)\n",
    "        if verbose:\n",
    "            print(\"Got an average of {}% {} way one-shot learning accuracy\".format(percent_correct,N))\n",
    "        return percent_correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAAFKCAYAAACaZ1R9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3340079d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loader = Siamese_Loader(X,Xval)\n",
    "\n",
    "def concat_images(X):\n",
    "    \"\"\"Concatenates a bnch of images into a big matrix for plotting purposes.\"\"\"\n",
    "    nc,h,w,_ = X.shape\n",
    "    X = X.reshape(nc,h,w)\n",
    "    n = np.ceil(np.sqrt(nc)).astype(\"int8\")\n",
    "    img = np.zeros((n*w,n*h))\n",
    "    x = 0\n",
    "    y = 0\n",
    "    for example in range(nc):\n",
    "        img[x*w:(x+1)*w,y*h:(y+1)*h] = X[example]\n",
    "        y += 1\n",
    "        if y >= n:\n",
    "            y = 0\n",
    "            x += 1\n",
    "    return img\n",
    "\n",
    "\n",
    "def plot_oneshot_task(pairs):\n",
    "    \"\"\"Takes a one-shot task given to a siamese net and  \"\"\"\n",
    "    fig,(ax1,ax2) = plt.subplots(2)\n",
    "    ax1.matshow(pairs[0][0].reshape(105,105),cmap='gray')\n",
    "    img = concat_images(pairs[1])\n",
    "    ax1.get_yaxis().set_visible(False)\n",
    "    ax1.get_xaxis().set_visible(False)\n",
    "    ax2.matshow(img,cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "#example of a one-shot learning task\n",
    "pairs, targets = loader.make_oneshot_task(25)\n",
    "plot_oneshot_task(pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 73.0909090909% 20 way one-shot learning accuracy\n",
      "iteration 0, training loss: 8.84,\n",
      "iteration 300, training loss: 8.84,\n",
      "iteration 600, training loss: 8.82,\n",
      "iteration 900, training loss: 8.81,\n",
      "iteration 1200, training loss: 8.85,\n",
      "iteration 1500, training loss: 8.89,\n",
      "iteration 1800, training loss: 8.86,\n",
      "iteration 2100, training loss: 8.88,\n",
      "iteration 2400, training loss: 8.81,\n",
      "iteration 2700, training loss: 8.81,\n",
      "iteration 3000, training loss: 8.82,\n",
      "iteration 3300, training loss: 8.79,\n",
      "iteration 3600, training loss: 8.83,\n",
      "iteration 3900, training loss: 8.87,\n",
      "iteration 4200, training loss: 8.77,\n",
      "iteration 4500, training loss: 8.81,\n",
      "iteration 4800, training loss: 8.82,\n",
      "iteration 5100, training loss: 8.88,\n",
      "iteration 5400, training loss: 8.86,\n",
      "iteration 5700, training loss: 8.85,\n",
      "iteration 6000, training loss: 8.86,\n",
      "iteration 6300, training loss: 8.91,\n",
      "iteration 6600, training loss: 8.85,\n",
      "iteration 6900, training loss: 8.81,\n",
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 74.1818181818% 20 way one-shot learning accuracy\n",
      "iteration 7200, training loss: 8.77,\n",
      "iteration 7500, training loss: 8.98,\n",
      "iteration 7800, training loss: 8.76,\n",
      "iteration 8100, training loss: 8.83,\n",
      "iteration 8400, training loss: 8.82,\n",
      "iteration 8700, training loss: 8.79,\n",
      "iteration 9000, training loss: 8.80,\n",
      "iteration 9300, training loss: 8.75,\n",
      "iteration 9600, training loss: 8.87,\n",
      "iteration 9900, training loss: 8.83,\n",
      "iteration 10200, training loss: 8.78,\n",
      "iteration 10500, training loss: 8.78,\n",
      "iteration 10800, training loss: 8.78,\n",
      "iteration 11100, training loss: 8.84,\n",
      "iteration 11400, training loss: 8.81,\n",
      "iteration 11700, training loss: 8.77,\n",
      "iteration 12000, training loss: 8.90,\n",
      "iteration 12300, training loss: 8.79,\n",
      "iteration 12600, training loss: 8.91,\n",
      "iteration 12900, training loss: 8.84,\n",
      "iteration 13200, training loss: 8.76,\n",
      "iteration 13500, training loss: 8.85,\n",
      "iteration 13800, training loss: 8.86,\n",
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 73.6363636364% 20 way one-shot learning accuracy\n",
      "iteration 14100, training loss: 8.78,\n",
      "iteration 14400, training loss: 8.79,\n",
      "iteration 14700, training loss: 8.93,\n",
      "iteration 15000, training loss: 8.81,\n",
      "iteration 15300, training loss: 8.89,\n",
      "iteration 15600, training loss: 8.79,\n",
      "iteration 15900, training loss: 8.88,\n",
      "iteration 16200, training loss: 8.87,\n",
      "iteration 16500, training loss: 8.79,\n",
      "iteration 16800, training loss: 8.80,\n",
      "iteration 17100, training loss: 8.86,\n",
      "iteration 17400, training loss: 8.77,\n",
      "iteration 17700, training loss: 8.75,\n",
      "iteration 18000, training loss: 8.86,\n",
      "iteration 18300, training loss: 8.88,\n",
      "iteration 18600, training loss: 8.94,\n",
      "iteration 18900, training loss: 8.77,\n",
      "iteration 19200, training loss: 8.78,\n",
      "iteration 19500, training loss: 8.75,\n",
      "iteration 19800, training loss: 8.76,\n",
      "iteration 20100, training loss: 8.75,\n",
      "iteration 20400, training loss: 8.84,\n",
      "iteration 20700, training loss: 8.78,\n",
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 74.7272727273% 20 way one-shot learning accuracy\n",
      "iteration 21000, training loss: 8.78,\n",
      "iteration 21300, training loss: 8.92,\n",
      "iteration 21600, training loss: 8.83,\n",
      "iteration 21900, training loss: 8.82,\n",
      "iteration 22200, training loss: 8.81,\n",
      "iteration 22500, training loss: 8.76,\n",
      "iteration 22800, training loss: 8.82,\n",
      "iteration 23100, training loss: 8.84,\n",
      "iteration 23400, training loss: 8.83,\n",
      "iteration 23700, training loss: 8.78,\n",
      "iteration 24000, training loss: 8.79,\n",
      "iteration 24300, training loss: 8.83,\n",
      "iteration 24600, training loss: 8.74,\n",
      "iteration 24900, training loss: 8.83,\n",
      "iteration 25200, training loss: 8.77,\n",
      "iteration 25500, training loss: 8.82,\n",
      "iteration 25800, training loss: 8.80,\n",
      "iteration 26100, training loss: 8.76,\n",
      "iteration 26400, training loss: 8.76,\n",
      "iteration 26700, training loss: 8.87,\n",
      "iteration 27000, training loss: 8.74,\n",
      "iteration 27300, training loss: 8.87,\n",
      "iteration 27600, training loss: 8.79,\n",
      "iteration 27900, training loss: 8.82,\n",
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 77.0909090909% 20 way one-shot learning accuracy\n",
      "saving\n",
      "iteration 28200, training loss: 8.81,\n",
      "iteration 28500, training loss: 8.77,\n",
      "iteration 28800, training loss: 8.83,\n",
      "iteration 29100, training loss: 8.80,\n",
      "iteration 29400, training loss: 8.78,\n",
      "iteration 29700, training loss: 8.76,\n",
      "iteration 30000, training loss: 8.80,\n",
      "iteration 30300, training loss: 8.79,\n",
      "iteration 30600, training loss: 8.86,\n",
      "iteration 30900, training loss: 8.77,\n",
      "iteration 31200, training loss: 8.73,\n",
      "iteration 31500, training loss: 8.74,\n",
      "iteration 31800, training loss: 8.83,\n",
      "iteration 32100, training loss: 8.76,\n",
      "iteration 32400, training loss: 8.76,\n",
      "iteration 32700, training loss: 8.85,\n",
      "iteration 33000, training loss: 8.78,\n",
      "iteration 33300, training loss: 8.77,\n",
      "iteration 33600, training loss: 8.82,\n",
      "iteration 33900, training loss: 8.74,\n",
      "iteration 34200, training loss: 8.85,\n",
      "iteration 34500, training loss: 8.75,\n",
      "iteration 34800, training loss: 8.76,\n",
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy\n",
      "iteration 35100, training loss: 9.09,\n",
      "iteration 35400, training loss: 9.03,\n",
      "iteration 35700, training loss: 8.87,\n",
      "iteration 36000, training loss: 8.75,\n",
      "iteration 36300, training loss: 9.11,\n",
      "iteration 36600, training loss: 8.75,\n",
      "iteration 36900, training loss: 8.78,\n",
      "iteration 37200, training loss: 8.81,\n",
      "iteration 37500, training loss: 8.76,\n",
      "iteration 37800, training loss: 8.82,\n",
      "iteration 38100, training loss: 8.91,\n",
      "iteration 38400, training loss: 8.79,\n",
      "iteration 38700, training loss: 8.77,\n",
      "iteration 39000, training loss: 8.82,\n",
      "iteration 39300, training loss: 8.80,\n",
      "iteration 39600, training loss: 8.82,\n",
      "iteration 39900, training loss: 8.75,\n",
      "iteration 40200, training loss: 8.90,\n",
      "iteration 40500, training loss: 8.84,\n",
      "iteration 40800, training loss: 8.82,\n",
      "iteration 41100, training loss: 8.84,\n",
      "iteration 41400, training loss: 8.73,\n",
      "iteration 41700, training loss: 8.82,\n",
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 75.2727272727% 20 way one-shot learning accuracy\n",
      "iteration 42000, training loss: 8.81,\n",
      "iteration 42300, training loss: 8.78,\n",
      "iteration 42600, training loss: 8.73,\n",
      "iteration 42900, training loss: 8.79,\n",
      "iteration 43200, training loss: 8.79,\n",
      "iteration 43500, training loss: 8.87,\n",
      "iteration 43800, training loss: 8.79,\n",
      "iteration 44100, training loss: 8.75,\n",
      "iteration 44400, training loss: 8.78,\n",
      "iteration 44700, training loss: 8.75,\n",
      "iteration 45000, training loss: 8.84,\n",
      "iteration 45300, training loss: 8.80,\n",
      "iteration 45600, training loss: 8.77,\n",
      "iteration 45900, training loss: 8.80,\n",
      "iteration 46200, training loss: 8.77,\n",
      "iteration 46500, training loss: 8.79,\n",
      "iteration 46800, training loss: 8.89,\n",
      "iteration 47100, training loss: 8.76,\n",
      "iteration 47400, training loss: 8.82,\n",
      "iteration 47700, training loss: 8.81,\n",
      "iteration 48000, training loss: 8.76,\n",
      "iteration 48300, training loss: 8.74,\n",
      "iteration 48600, training loss: 8.87,\n",
      "iteration 48900, training loss: 8.76,\n",
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 74.3636363636% 20 way one-shot learning accuracy\n",
      "iteration 49200, training loss: 8.78,\n",
      "iteration 49500, training loss: 8.76,\n",
      "iteration 49800, training loss: 8.78,\n",
      "iteration 50100, training loss: 8.75,\n",
      "iteration 50400, training loss: 8.75,\n",
      "iteration 50700, training loss: 8.76,\n",
      "iteration 51000, training loss: 8.83,\n",
      "iteration 51300, training loss: 8.80,\n",
      "iteration 51600, training loss: 8.82,\n",
      "iteration 51900, training loss: 8.77,\n",
      "iteration 52200, training loss: 8.83,\n",
      "iteration 52500, training loss: 8.73,\n",
      "iteration 52800, training loss: 9.03,\n",
      "iteration 53100, training loss: 8.74,\n",
      "iteration 53400, training loss: 8.77,\n",
      "iteration 53700, training loss: 8.84,\n",
      "iteration 54000, training loss: 8.81,\n",
      "iteration 54300, training loss: 8.72,\n",
      "iteration 54600, training loss: 8.80,\n",
      "iteration 54900, training loss: 8.92,\n",
      "iteration 55200, training loss: 8.79,\n",
      "iteration 55500, training loss: 8.76,\n",
      "iteration 55800, training loss: 8.79,\n",
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 73.6363636364% 20 way one-shot learning accuracy\n",
      "iteration 56100, training loss: 8.77,\n",
      "iteration 56400, training loss: 8.72,\n",
      "iteration 56700, training loss: 8.84,\n",
      "iteration 57000, training loss: 8.75,\n",
      "iteration 57300, training loss: 8.74,\n",
      "iteration 57600, training loss: 8.85,\n",
      "iteration 57900, training loss: 8.90,\n",
      "iteration 58200, training loss: 8.83,\n",
      "iteration 58500, training loss: 8.81,\n",
      "iteration 58800, training loss: 8.77,\n",
      "iteration 59100, training loss: 8.80,\n",
      "iteration 59400, training loss: 8.88,\n",
      "iteration 59700, training loss: 8.81,\n",
      "iteration 60000, training loss: 8.75,\n",
      "iteration 60300, training loss: 8.75,\n",
      "iteration 60600, training loss: 8.81,\n",
      "iteration 60900, training loss: 8.76,\n",
      "iteration 61200, training loss: 8.80,\n",
      "iteration 61500, training loss: 8.78,\n",
      "iteration 61800, training loss: 8.75,\n",
      "iteration 62100, training loss: 8.75,\n",
      "iteration 62400, training loss: 8.72,\n",
      "iteration 62700, training loss: 8.89,\n",
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 73.2727272727% 20 way one-shot learning accuracy\n",
      "iteration 63000, training loss: 8.75,\n",
      "iteration 63300, training loss: 8.90,\n",
      "iteration 63600, training loss: 8.96,\n",
      "iteration 63900, training loss: 8.84,\n",
      "iteration 64200, training loss: 8.76,\n",
      "iteration 64500, training loss: 8.75,\n",
      "iteration 64800, training loss: 8.74,\n",
      "iteration 65100, training loss: 8.81,\n",
      "iteration 65400, training loss: 8.83,\n",
      "iteration 65700, training loss: 8.79,\n",
      "iteration 66000, training loss: 8.83,\n",
      "iteration 66300, training loss: 8.76,\n",
      "iteration 66600, training loss: 8.75,\n",
      "iteration 66900, training loss: 8.84,\n",
      "iteration 67200, training loss: 8.80,\n",
      "iteration 67500, training loss: 8.84,\n",
      "iteration 67800, training loss: 8.78,\n",
      "iteration 68100, training loss: 8.80,\n",
      "iteration 68400, training loss: 8.79,\n",
      "iteration 68700, training loss: 8.74,\n",
      "iteration 69000, training loss: 8.83,\n",
      "iteration 69300, training loss: 8.74,\n",
      "iteration 69600, training loss: 8.76,\n",
      "iteration 69900, training loss: 8.94,\n",
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 72.9090909091% 20 way one-shot learning accuracy\n",
      "iteration 70200, training loss: 8.83,\n",
      "iteration 70500, training loss: 8.75,\n",
      "iteration 70800, training loss: 8.80,\n",
      "iteration 71100, training loss: 8.95,\n",
      "iteration 71400, training loss: 8.89,\n",
      "iteration 71700, training loss: 8.80,\n",
      "iteration 72000, training loss: 8.85,\n",
      "iteration 72300, training loss: 8.78,\n",
      "iteration 72600, training loss: 8.77,\n",
      "iteration 72900, training loss: 8.75,\n",
      "iteration 73200, training loss: 8.75,\n",
      "iteration 73500, training loss: 8.78,\n",
      "iteration 73800, training loss: 8.83,\n",
      "iteration 74100, training loss: 8.81,\n",
      "iteration 74400, training loss: 8.81,\n",
      "iteration 74700, training loss: 8.80,\n",
      "iteration 75000, training loss: 8.76,\n",
      "iteration 75300, training loss: 8.78,\n",
      "iteration 75600, training loss: 8.73,\n",
      "iteration 75900, training loss: 8.85,\n",
      "iteration 76200, training loss: 8.76,\n",
      "iteration 76500, training loss: 8.80,\n",
      "iteration 76800, training loss: 8.74,\n",
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 74.1818181818% 20 way one-shot learning accuracy\n",
      "iteration 77100, training loss: 8.74,\n",
      "iteration 77400, training loss: 8.76,\n",
      "iteration 77700, training loss: 9.00,\n",
      "iteration 78000, training loss: 8.75,\n",
      "iteration 78300, training loss: 8.83,\n",
      "iteration 78600, training loss: 8.78,\n",
      "iteration 78900, training loss: 8.74,\n",
      "iteration 79200, training loss: 8.72,\n",
      "iteration 79500, training loss: 8.85,\n",
      "iteration 79800, training loss: 8.87,\n",
      "iteration 80100, training loss: 8.77,\n",
      "iteration 80400, training loss: 8.75,\n",
      "iteration 80700, training loss: 8.73,\n",
      "iteration 81000, training loss: 8.73,\n",
      "iteration 81300, training loss: 8.76,\n",
      "iteration 81600, training loss: 8.75,\n",
      "iteration 81900, training loss: 8.77,\n",
      "iteration 82200, training loss: 8.73,\n",
      "iteration 82500, training loss: 8.77,\n",
      "iteration 82800, training loss: 8.78,\n",
      "iteration 83100, training loss: 8.86,\n",
      "iteration 83400, training loss: 8.74,\n",
      "iteration 83700, training loss: 8.81,\n",
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy\n",
      "iteration 84000, training loss: 8.79,\n",
      "iteration 84300, training loss: 8.78,\n",
      "iteration 84600, training loss: 8.74,\n",
      "iteration 84900, training loss: 8.82,\n",
      "iteration 85200, training loss: 8.77,\n",
      "iteration 85500, training loss: 8.76,\n",
      "iteration 85800, training loss: 8.78,\n",
      "iteration 86100, training loss: 8.81,\n",
      "iteration 86400, training loss: 8.85,\n",
      "iteration 86700, training loss: 8.77,\n",
      "iteration 87000, training loss: 8.80,\n",
      "iteration 87300, training loss: 8.74,\n",
      "iteration 87600, training loss: 8.74,\n",
      "iteration 87900, training loss: 8.78,\n",
      "iteration 88200, training loss: 8.73,\n",
      "iteration 88500, training loss: 8.74,\n",
      "iteration 88800, training loss: 8.86,\n",
      "iteration 89100, training loss: 8.78,\n",
      "iteration 89400, training loss: 8.77,\n",
      "iteration 89700, training loss: 8.86,\n",
      "iteration 90000, training loss: 8.81,\n",
      "iteration 90300, training loss: 8.75,\n",
      "iteration 90600, training loss: 8.75,\n",
      "iteration 90900, training loss: 8.75,\n",
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy\n",
      "iteration 91200, training loss: 8.76,\n",
      "iteration 91500, training loss: 8.81,\n",
      "iteration 91800, training loss: 8.76,\n",
      "iteration 92100, training loss: 8.82,\n",
      "iteration 92400, training loss: 8.79,\n",
      "iteration 92700, training loss: 8.75,\n",
      "iteration 93000, training loss: 8.83,\n",
      "iteration 93300, training loss: 8.77,\n",
      "iteration 93600, training loss: 8.76,\n",
      "iteration 93900, training loss: 8.92,\n",
      "iteration 94200, training loss: 8.73,\n",
      "iteration 94500, training loss: 8.81,\n",
      "iteration 94800, training loss: 8.79,\n",
      "iteration 95100, training loss: 8.75,\n",
      "iteration 95400, training loss: 8.86,\n",
      "iteration 95700, training loss: 8.79,\n",
      "iteration 96000, training loss: 8.82,\n",
      "iteration 96300, training loss: 8.80,\n",
      "iteration 96600, training loss: 8.80,\n",
      "iteration 96900, training loss: 8.74,\n",
      "iteration 97200, training loss: 8.73,\n",
      "iteration 97500, training loss: 8.77,\n",
      "iteration 97800, training loss: 8.74,\n",
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 74.5454545455% 20 way one-shot learning accuracy\n",
      "iteration 98100, training loss: 8.89,\n",
      "iteration 98400, training loss: 8.75,\n",
      "iteration 98700, training loss: 8.77,\n",
      "iteration 99000, training loss: 8.74,\n",
      "iteration 99300, training loss: 8.75,\n",
      "iteration 99600, training loss: 8.88,\n",
      "iteration 99900, training loss: 8.73,\n",
      "iteration 100200, training loss: 8.79,\n",
      "iteration 100500, training loss: 8.75,\n",
      "iteration 100800, training loss: 8.76,\n",
      "iteration 101100, training loss: 8.79,\n",
      "iteration 101400, training loss: 8.75,\n",
      "iteration 101700, training loss: 8.74,\n",
      "iteration 102000, training loss: 8.81,\n",
      "iteration 102300, training loss: 8.74,\n",
      "iteration 102600, training loss: 8.83,\n",
      "iteration 102900, training loss: 8.76,\n",
      "iteration 103200, training loss: 8.73,\n",
      "iteration 103500, training loss: 8.80,\n",
      "iteration 103800, training loss: 8.72,\n",
      "iteration 104100, training loss: 8.78,\n",
      "iteration 104400, training loss: 8.82,\n",
      "iteration 104700, training loss: 8.80,\n",
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 73.0909090909% 20 way one-shot learning accuracy\n",
      "iteration 105000, training loss: 8.79,\n",
      "iteration 105300, training loss: 8.83,\n",
      "iteration 105600, training loss: 8.88,\n",
      "iteration 105900, training loss: 8.78,\n",
      "iteration 106200, training loss: 8.74,\n",
      "iteration 106500, training loss: 8.77,\n",
      "iteration 106800, training loss: 8.76,\n",
      "iteration 107100, training loss: 8.79,\n",
      "iteration 107400, training loss: 8.77,\n",
      "iteration 107700, training loss: 8.76,\n",
      "iteration 108000, training loss: 8.74,\n",
      "iteration 108300, training loss: 8.75,\n",
      "iteration 108600, training loss: 8.84,\n",
      "iteration 108900, training loss: 8.76,\n",
      "iteration 109200, training loss: 8.82,\n",
      "iteration 109500, training loss: 8.75,\n",
      "iteration 109800, training loss: 8.83,\n",
      "iteration 110100, training loss: 8.74,\n",
      "iteration 110400, training loss: 8.74,\n",
      "iteration 110700, training loss: 8.86,\n",
      "iteration 111000, training loss: 8.90,\n",
      "iteration 111300, training loss: 8.77,\n",
      "iteration 111600, training loss: 8.76,\n",
      "iteration 111900, training loss: 8.79,\n",
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 75.2727272727% 20 way one-shot learning accuracy\n",
      "iteration 112200, training loss: 8.82,\n",
      "iteration 112500, training loss: 8.73,\n",
      "iteration 112800, training loss: 8.76,\n",
      "iteration 113100, training loss: 8.76,\n",
      "iteration 113400, training loss: 8.83,\n",
      "iteration 113700, training loss: 8.77,\n",
      "iteration 114000, training loss: 8.90,\n",
      "iteration 114300, training loss: 8.74,\n",
      "iteration 114600, training loss: 8.79,\n",
      "iteration 114900, training loss: 8.78,\n",
      "iteration 115200, training loss: 8.81,\n",
      "iteration 115500, training loss: 8.84,\n",
      "iteration 115800, training loss: 8.87,\n",
      "iteration 116100, training loss: 8.92,\n",
      "iteration 116400, training loss: 8.80,\n",
      "iteration 116700, training loss: 8.74,\n",
      "iteration 117000, training loss: 8.80,\n",
      "iteration 117300, training loss: 8.76,\n",
      "iteration 117600, training loss: 8.74,\n",
      "iteration 117900, training loss: 8.80,\n",
      "iteration 118200, training loss: 8.87,\n",
      "iteration 118500, training loss: 8.73,\n",
      "iteration 118800, training loss: 8.79,\n",
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 76.3636363636% 20 way one-shot learning accuracy\n",
      "iteration 119100, training loss: 8.73,\n",
      "iteration 119400, training loss: 8.73,\n",
      "iteration 119700, training loss: 8.75,\n",
      "iteration 120000, training loss: 8.85,\n",
      "iteration 120300, training loss: 8.86,\n",
      "iteration 120600, training loss: 8.74,\n",
      "iteration 120900, training loss: 8.76,\n",
      "iteration 121200, training loss: 8.73,\n",
      "iteration 121500, training loss: 8.74,\n",
      "iteration 121800, training loss: 8.75,\n",
      "iteration 122100, training loss: 8.74,\n",
      "iteration 122400, training loss: 8.73,\n",
      "iteration 122700, training loss: 8.80,\n",
      "iteration 123000, training loss: 8.76,\n",
      "iteration 123300, training loss: 8.81,\n",
      "iteration 123600, training loss: 8.77,\n",
      "iteration 123900, training loss: 8.73,\n",
      "iteration 124200, training loss: 8.75,\n",
      "iteration 124500, training loss: 8.80,\n",
      "iteration 124800, training loss: 8.74,\n",
      "iteration 125100, training loss: 8.78,\n",
      "iteration 125400, training loss: 8.75,\n",
      "iteration 125700, training loss: 8.86,\n",
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 74.1818181818% 20 way one-shot learning accuracy\n",
      "iteration 126000, training loss: 8.75,\n",
      "iteration 126300, training loss: 8.73,\n",
      "iteration 126600, training loss: 8.74,\n",
      "iteration 126900, training loss: 8.74,\n",
      "iteration 127200, training loss: 8.74,\n",
      "iteration 127500, training loss: 8.76,\n",
      "iteration 127800, training loss: 8.81,\n",
      "iteration 128100, training loss: 8.77,\n",
      "iteration 128400, training loss: 8.80,\n",
      "iteration 128700, training loss: 8.83,\n",
      "iteration 129000, training loss: 8.73,\n",
      "iteration 129300, training loss: 8.83,\n",
      "iteration 129600, training loss: 8.87,\n",
      "iteration 129900, training loss: 8.75,\n",
      "iteration 130200, training loss: 8.73,\n",
      "iteration 130500, training loss: 8.73,\n",
      "iteration 130800, training loss: 8.75,\n",
      "iteration 131100, training loss: 8.79,\n",
      "iteration 131400, training loss: 8.74,\n",
      "iteration 131700, training loss: 8.74,\n",
      "iteration 132000, training loss: 8.81,\n",
      "iteration 132300, training loss: 8.73,\n",
      "iteration 132600, training loss: 8.75,\n",
      "iteration 132900, training loss: 8.81,\n",
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 71.8181818182% 20 way one-shot learning accuracy\n",
      "iteration 133200, training loss: 8.81,\n",
      "iteration 133500, training loss: 8.75,\n",
      "iteration 133800, training loss: 8.85,\n",
      "iteration 134100, training loss: 8.75,\n",
      "iteration 134400, training loss: 8.79,\n",
      "iteration 134700, training loss: 8.76,\n",
      "iteration 135000, training loss: 8.79,\n",
      "iteration 135300, training loss: 8.73,\n",
      "iteration 135600, training loss: 8.76,\n",
      "iteration 135900, training loss: 8.77,\n",
      "iteration 136200, training loss: 8.73,\n",
      "iteration 136500, training loss: 8.73,\n",
      "iteration 136800, training loss: 8.78,\n",
      "iteration 137100, training loss: 8.76,\n",
      "iteration 137400, training loss: 8.76,\n",
      "iteration 137700, training loss: 8.78,\n",
      "iteration 138000, training loss: 8.77,\n",
      "iteration 138300, training loss: 8.87,\n",
      "iteration 138600, training loss: 8.72,\n",
      "iteration 138900, training loss: 8.75,\n",
      "iteration 139200, training loss: 8.85,\n",
      "iteration 139500, training loss: 8.72,\n",
      "iteration 139800, training loss: 8.74,\n",
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 74.1818181818% 20 way one-shot learning accuracy\n",
      "iteration 140100, training loss: 8.86,\n",
      "iteration 140400, training loss: 8.76,\n",
      "iteration 140700, training loss: 8.81,\n",
      "iteration 141000, training loss: 8.72,\n",
      "iteration 141300, training loss: 8.76,\n",
      "iteration 141600, training loss: 8.72,\n",
      "iteration 141900, training loss: 8.84,\n",
      "iteration 142200, training loss: 8.78,\n",
      "iteration 142500, training loss: 8.85,\n",
      "iteration 142800, training loss: 8.86,\n",
      "iteration 143100, training loss: 8.78,\n",
      "iteration 143400, training loss: 8.76,\n",
      "iteration 143700, training loss: 8.72,\n",
      "iteration 144000, training loss: 8.77,\n",
      "iteration 144300, training loss: 8.83,\n",
      "iteration 144600, training loss: 8.76,\n",
      "iteration 144900, training loss: 8.80,\n",
      "iteration 145200, training loss: 8.73,\n",
      "iteration 145500, training loss: 8.74,\n",
      "iteration 145800, training loss: 8.80,\n",
      "iteration 146100, training loss: 8.74,\n",
      "iteration 146400, training loss: 8.91,\n",
      "iteration 146700, training loss: 8.75,\n",
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy\n",
      "iteration 147000, training loss: 8.77,\n",
      "iteration 147300, training loss: 8.74,\n",
      "iteration 147600, training loss: 8.77,\n",
      "iteration 147900, training loss: 8.77,\n",
      "iteration 148200, training loss: 8.83,\n",
      "iteration 148500, training loss: 8.89,\n",
      "iteration 148800, training loss: 8.85,\n",
      "iteration 149100, training loss: 8.79,\n",
      "iteration 149400, training loss: 8.81,\n",
      "iteration 149700, training loss: 8.76,\n",
      "iteration 150000, training loss: 8.75,\n",
      "iteration 150300, training loss: 8.74,\n",
      "iteration 150600, training loss: 8.77,\n",
      "iteration 150900, training loss: 8.74,\n",
      "iteration 151200, training loss: 8.96,\n",
      "iteration 151500, training loss: 8.82,\n",
      "iteration 151800, training loss: 8.75,\n",
      "iteration 152100, training loss: 8.74,\n",
      "iteration 152400, training loss: 8.74,\n",
      "iteration 152700, training loss: 8.77,\n",
      "iteration 153000, training loss: 8.78,\n",
      "iteration 153300, training loss: 8.83,\n",
      "iteration 153600, training loss: 8.87,\n",
      "iteration 153900, training loss: 8.75,\n",
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 77.8181818182% 20 way one-shot learning accuracy\n",
      "saving\n",
      "iteration 154200, training loss: 8.79,\n",
      "iteration 154500, training loss: 8.79,\n",
      "iteration 154800, training loss: 8.73,\n",
      "iteration 155100, training loss: 8.75,\n",
      "iteration 155400, training loss: 8.80,\n",
      "iteration 155700, training loss: 8.81,\n",
      "iteration 156000, training loss: 8.76,\n",
      "iteration 156300, training loss: 8.72,\n",
      "iteration 156600, training loss: 8.80,\n",
      "iteration 156900, training loss: 8.72,\n",
      "iteration 157200, training loss: 8.73,\n",
      "iteration 157500, training loss: 8.83,\n",
      "iteration 157800, training loss: 8.73,\n",
      "iteration 158100, training loss: 8.74,\n",
      "iteration 158400, training loss: 8.74,\n",
      "iteration 158700, training loss: 8.79,\n",
      "iteration 159000, training loss: 8.76,\n",
      "iteration 159300, training loss: 8.74,\n",
      "iteration 159600, training loss: 8.75,\n",
      "iteration 159900, training loss: 8.74,\n",
      "iteration 160200, training loss: 8.82,\n",
      "iteration 160500, training loss: 8.72,\n",
      "iteration 160800, training loss: 8.80,\n",
      "Evaluating model on 550 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 74.1818181818% 20 way one-shot learning accuracy\n",
      "iteration 161100, training loss: 8.74,\n",
      "iteration 161400, training loss: 8.77,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9d9c987ba389>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m900000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msiamese_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mevaluate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_oneshot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msiamese_net\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_way\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1618\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2073\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2075\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2076\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate_every = 7000\n",
    "loss_every=300\n",
    "batch_size = 32\n",
    "N_way = 20\n",
    "n_val = 550\n",
    "siamese_net.load_weights(\"/home/soren/keras-oneshot/weights\")\n",
    "best = 76.0\n",
    "for i in range(900000):\n",
    "    (inputs,targets)=loader.get_batch(batch_size)\n",
    "    loss=siamese_net.train_on_batch(inputs,targets)\n",
    "    if i % evaluate_every == 0:\n",
    "        val_acc = loader.test_oneshot(siamese_net,N_way,n_val,verbose=True)\n",
    "        if val_acc >= best:\n",
    "            print(\"saving\")\n",
    "            siamese_net.save('/home/soren/keras-oneshot/weights')\n",
    "            best=val_acc\n",
    "\n",
    "    if i % loss_every == 0:\n",
    "        print(\"iteration {}, training loss: {:.2f},\".format(i,loss))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on 1500 unique 20 way one-shot learning tasks ...\n",
      "Got an average of 73.4% 20 way one-shot learning accuracy\n"
     ]
    }
   ],
   "source": [
    "siamese_net.load_weights(\"/home/soren/keras-oneshot/weights\")\n",
    "val_acc = loader.test_oneshot(siamese_net,20,1500,verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

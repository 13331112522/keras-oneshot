{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:45: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python3.5/dist-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38951745"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import numpy.random as rng\n",
    "import numpy as np\n",
    "import os\n",
    "import dill as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def W_init(shape,name=None):\n",
    "    \"\"\"Initialize weights as in paper\"\"\"\n",
    "    values = rng.normal(loc=0,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)\n",
    "#//TODO: figure out how to initialize layer biases in keras.\n",
    "def b_init(shape,name=None):\n",
    "    \"\"\"Initialize bias as in paper\"\"\"\n",
    "    values=rng.normal(loc=0.5,scale=1e-2,size=shape)\n",
    "    return K.variable(values,name=name)\n",
    "\n",
    "input_shape = (105, 105, 1)\n",
    "left_input = Input(input_shape)\n",
    "right_input = Input(input_shape)\n",
    "#build convnet to use in each siamese 'leg'\n",
    "convnet = Sequential()\n",
    "convnet.add(Conv2D(64,(10,10),activation='relu',input_shape=input_shape,\n",
    "                   kernel_initializer=W_init,kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(Conv2D(128,(7,7),activation='relu',\n",
    "                   kernel_regularizer=l2(2e-4),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(Conv2D(128,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(Conv2D(256,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))\n",
    "convnet.add(Flatten())\n",
    "convnet.add(Dense(4096,activation=\"sigmoid\",kernel_regularizer=l2(1e-3),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "#encode each of the two inputs into a vector with the convnet\n",
    "encoded_l = convnet(left_input)\n",
    "encoded_r = convnet(right_input)\n",
    "#merge two encoded inputs with the l1 distance between them\n",
    "L1_distance = lambda x: K.abs(x[0]-x[1])\n",
    "both = merge([encoded_l,encoded_r], mode = L1_distance, output_shape=lambda x: x[0])\n",
    "prediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(both)\n",
    "siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "#optimizer = SGD(0.0004,momentum=0.6,nesterov=True,decay=0.0003)\n",
    "\n",
    "optimizer = Adam(0.00006)\n",
    "#//TODO: get layerwise learning rates and momentum annealing scheme described in paperworking\n",
    "siamese_net.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n",
    "\n",
    "siamese_net.count_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "The data is pickled as an N_classes x n_examples x width x height array, and there is an accompanyng dictionary to specify which indexes belong to which languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a4fc1b1812c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m\"/home/soren/keras-oneshot/train.pickle\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m\"/home/soren/keras-oneshot/val.pickle\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mXval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/dill/dill.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[0mpik\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[0mpik\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_main\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_main_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m     \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpik\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_main_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__name__'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[1;31m# point obj class to main\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3.5/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[1;31m# decode input (taking the buffer into account)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m         \u001b[1;31m# keep undecoded input until the next call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "with open( \"/home/soren/keras-oneshot/train.pickle\",\"r\") as f:\n",
    "    (X,c) = pickle.load(f)\n",
    "\n",
    "with open( \"/home/soren/keras-oneshot/val.pickle\",\"r\") as f:\n",
    "    (Xval,cval) = pickle.load(f)\n",
    "    \n",
    "print(\"training alphabets\")\n",
    "print(c.keys())\n",
    "print(\"validation alphabets:\")\n",
    "print(cval.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Siamese_Loader:\n",
    "    \"\"\"For loading batches and testing tasks to a siamese net\"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.data = {}\n",
    "        self.categories = {}\n",
    "        self.info = {}\n",
    "        with open(os.path.join(path,\"train.pickle\"),\"r\") as f:\n",
    "            (X,c) = pickle.load(f)\n",
    "            self.data[\"train\"] = X\n",
    "            self.categories[\"train\"] = c\n",
    "        with open(os.path.join(path,\"val.pickle\"),\"r\") as f:\n",
    "            (X,c) = pickle.load(f)\n",
    "            self.data[\"val\"] = X\n",
    "            self.categories[\"val\"] = c\n",
    "            \n",
    "        self.n_classes,self.n_examples,self.w,self.h = self.data[\"train\"].shape\n",
    "        self.n_val,self.n_ex_val,_,_ = self.data['val'].shape\n",
    "\n",
    "    def get_batch(self,n,s=\"train\"):\n",
    "        \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
    "        X=self.data[s]\n",
    "        categories = rng.choice(self.n_classes,size=(n,),replace=False)\n",
    "        pairs=[np.zeros((n, self.h, self.w,1)) for i in range(2)]\n",
    "        targets=np.zeros((n,))\n",
    "        targets[n//2:] = 1\n",
    "        for i in range(n):\n",
    "            category = categories[i]\n",
    "            idx_1 = rng.randint(0,self.n_examples)\n",
    "            pairs[0][i,:,:,:] = X[category,idx_1].reshape(self.w,self.h,1)\n",
    "            idx_2 = rng.randint(0,self.n_examples)\n",
    "            #pick images of same class for 1st half, different for 2nd\n",
    "            category_2 = category if i >= n//2 else (category + rng.randint(1,self.n_classes)) % self.n_classes\n",
    "            pairs[1][i,:,:,:] = X[category_2,idx_2].reshape(self.w,self.h,1)\n",
    "        return pairs, targets\n",
    "    \n",
    "\n",
    "    def make_oneshot_task(self,N,s=\"val\",language=None):\n",
    "        \"\"\"Create pairs of test image, support set for testing N way one-shot learning. \"\"\"\n",
    "        X=self.data[s]\n",
    "        n_classes, n_examples = X.shape[0],X.shape[1]\n",
    "        if language is not None:\n",
    "            low, high = self.categories[s][language]\n",
    "            if N > high - low:\n",
    "                raise ValueError(\"This language ({}) has less than {} letters\".format(language, N))\n",
    "            categories = rng.choice(range(low,high),size=(N,),replace=False)\n",
    "            indices = rng.randint(0,self.n_examples,size=(N,))\n",
    "            \n",
    "        else:#if no language specified just pick a bunch of random letters\n",
    "            categories = rng.choice(range(n_classes),size=(N,),replace=False)            \n",
    "        true_category = categories[0]\n",
    "        ex1, ex2 = rng.choice(n_examples,replace=False,size=(2,))\n",
    "        test_image = np.asarray([X[true_category,ex1,:,:]]*N).reshape(N,self.w,self.h,1)\n",
    "        support_set = X[categories,indices,:,:]\n",
    "        support_set[0,:,:] = X[true_category,ex2]\n",
    "        support_set = support_set.reshape(N,self.w,self.h,1)\n",
    "        targets = np.zeros((N,))\n",
    "        targets[0] = 1\n",
    "        targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
    "        pairs = [test_image,support_set]\n",
    "\n",
    "        return pairs, targets\n",
    "    \n",
    "    def test_oneshot(self,model,N,k,s=\"val\",verbose=0):\n",
    "        \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
    "        n_correct = 0\n",
    "        if verbose:\n",
    "            print(\"Evaluating model on {} unique {} way one-shot learning tasks ...\".format(k,N))\n",
    "        for i in range(k):\n",
    "            inputs, targets = self.make_oneshot_task(N,s)\n",
    "            probs = model.predict(inputs)\n",
    "            if np.argmax(probs) == np.argmax(targets):\n",
    "                n_correct+=1\n",
    "        percent_correct = (100.0*n_correct / k)\n",
    "        if verbose:\n",
    "            print(\"Got an average of {}% {} way one-shot learning accuracy\".format(percent_correct,N))\n",
    "        return percent_correct\n",
    "\n",
    "loader = Siamese_Loader(\"/home/soren/keras-oneshot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def concat_images(X):\n",
    "    \"\"\"Concatenates a bunch of images into a big matrix for plotting purposes.\"\"\"\n",
    "    nc,h,w,_ = X.shape\n",
    "    X = X.reshape(nc,h,w)\n",
    "    n = np.ceil(np.sqrt(nc)).astype(\"int8\")\n",
    "    img = np.zeros((n*w,n*h))\n",
    "    x = 0\n",
    "    y = 0\n",
    "    for example in range(nc):\n",
    "        img[x*w:(x+1)*w,y*h:(y+1)*h] = X[example]\n",
    "        y += 1\n",
    "        if y >= n:\n",
    "            y = 0\n",
    "            x += 1\n",
    "    return img\n",
    "\n",
    "\n",
    "def plot_oneshot_task(pairs):\n",
    "    \"\"\"Takes a one-shot task given to a siamese net and  \"\"\"\n",
    "    fig,(ax1,ax2) = plt.subplots(2)\n",
    "    ax1.matshow(pairs[0][0].reshape(105,105),cmap='gray')\n",
    "    img = concat_images(pairs[1])\n",
    "    ax1.get_yaxis().set_visible(False)\n",
    "    ax1.get_xaxis().set_visible(False)\n",
    "    ax2.matshow(img,cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "#example of a one-shot learning task\n",
    "pairs, targets = loader.make_oneshot_task(20,\"train\",\"Japanese_(katakana)\")\n",
    "plot_oneshot_task(pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training loop\n",
    "evaluate_every = 500 # interval for evaluating on one-shot tasks\n",
    "loss_every=50 # interval for printing loss (iterations)\n",
    "batch_size = 32 \n",
    "n_iter = 900000 \n",
    "N_way = 20 # how many classes for testing one-shot tasks>\n",
    "n_val = 250 #how mahy one-shot tasks to validate on?\n",
    "best = 9999\n",
    "#siamese_net.load_weights(\"/home/soren/keras-oneshot/weights\")\n",
    "for i in range(1, n_iter):\n",
    "    (inputs,targets)=loader.get_batch(batch_size)\n",
    "    loss=siamese_net.train_on_batch(inputs,targets)\n",
    "    if i % evaluate_every == 0:\n",
    "        val_acc = loader.test_oneshot(siamese_net,N_way,n_val,verbose=True)\n",
    "        if val_acc >= best:\n",
    "            print(\"saving\")\n",
    "            siamese_net.save('/home/soren/keras-oneshot/weights')\n",
    "            best=val_acc\n",
    "\n",
    "    if i % loss_every == 0:\n",
    "        print(\"iteration {}, training loss: {:.2f},\".format(i,loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nearest_neighbour(pairs,targets):\n",
    "    \"\"\"Quick function to classify a pair of images using 1 nearest neighbour\"\"\"\n",
    "    L2_distances = np.zeros_like(targets)\n",
    "    for i in range(len(targets)):\n",
    "        L2_distances[i] = np.sum(np.sqrt(pairs[0][i]**2 - pairs[1][i]**2))\n",
    "    if np.argmin(L2_distances) == np.argmax(targets):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def test_nn_accuracy(N_ways,n_trials,loader):\n",
    "    print(\"Evaluating nearest neighbour on {} unique {} way one-shot learning tasks ...\".format(n_trials,N_ways))\n",
    "\n",
    "    n_right = 0\n",
    "    \n",
    "    for i in range(n_trials):\n",
    "        pairs,targets = loader.make_oneshot_task(N_ways,\"val\")\n",
    "        correct = nearest_neighbour(pairs,targets)\n",
    "        n_right+= correct\n",
    "    return 100.0 * n_right / n_trials\n",
    "\n",
    "#training loop\n",
    "siamese_net.load_weights(\"/home/soren/keras-oneshot/weights\")\n",
    "ways = np.arange(1,60,2)\n",
    "val_accs, train_accs,nn_accs = [],[],[]\n",
    "print(\"?\")\n",
    "trials = 450\n",
    "for N in ways:\n",
    "    val_accs.append(loader.test_oneshot(siamese_net,N,trials,\"val\",verbose=True))\n",
    "    train_accs.append(loader.test_oneshot(siamese_net,N,trials,\"train\",verbose=True))\n",
    "    nn_accs.append(test_nn_accuracy(N,trials,loader))\n",
    "import seaborn as sns\n",
    "plt.plot(ways,val_accs,\"m\")\n",
    "plt.plot(ways,train_accs,\"y\")\n",
    "plt.plot(ways,nn_accs,\"c\")\n",
    "\n",
    "plt.plot(ways,100.0/ways,\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1)\n",
    "ax.plot(ways,val_accs,\"m\",label=\"Siamese(val set)\")\n",
    "ax.plot(ways,train_accs,\"y\",label=\"Siamese(train set)\")\n",
    "plt.plot(ways,nn_accs,label=\"Nearest neighbour\")\n",
    "\n",
    "ax.plot(ways,100.0/ways,\"g\",label=\"Random guessing\")\n",
    "plt.xlabel(\"Number of possible classes in one-shot tasks\")\n",
    "plt.ylabel(\"% Accuracy\")\n",
    "plt.title(\"Omiglot One-Shot Learning Performance of a Siamese Network\")\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "inputs,targets = loader.make_oneshot_task(20,\"val\")\n",
    "plt.show()\n",
    "\n",
    "print(inputs[0].shape)\n",
    "plot_oneshot_task(inputs)\n",
    "p=siamese_net.predict(inputs)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "a=test_nn_accuracy(3,500,loader)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
